{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化\n",
    "optimizerを用いたパラメータの最適化(学習)について。\n",
    "\n",
    "# パラメータの最適化\n",
    "モデルに対して、訓練データを用いてパラメータを最適化する。  \n",
    "モデルの訓練は反復的なプロセスとなる。  \n",
    "1. モデルは出力を計算し、損失を求める。  \n",
    "2. 各パラメータに対して、損失に対する偏微分の値を計算する。  \n",
    "3. 勾配降下法に基づいてパラメータを最適化する。  \n",
    "4. 各イテレーション(エポック)で繰り返す。  \n",
    "\n",
    "最適化のプロセスは次の動画も参照[最適化プロセス](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コードの準備\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data,  batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ハイパーパラメータ\n",
    "ハイパーパラメータとは、モデルの最適化プロセスを制御するためのパラメータである。  \n",
    "ハイパーパラメータの値は、モデルの学習や収束率に影響する。  \n",
    "詳細は[ハイパーパラメータの詳細](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)を参照  \n",
    "\n",
    "\n",
    "今回は、訓練用のハイパーパラメータとして、\n",
    "1. **Number of Epochs** : epoch数  \n",
    "2. **Batch Size** : ミニバッチサイズを構成するデータサイズ  \n",
    "3. **Leaning Rate** : パラメータの更新の係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化ループ\n",
    "ハイパーパラメータの設定後、訓練で、最適化のループを回すことで、モデルを最適化する。  \n",
    "最適化ループの1回のイテレーションのことを**エポック(epoch)**という。  \n",
    "各エポックは以下の2種類のループから構成される。  \n",
    "1. **訓練ループ** : データセットに対して訓練を実行しパラメータを収束させる。  \n",
    "2. **検証/テストループ** : テストデータでモデルを評価し、性能を評価する  \n",
    "\n",
    "# 損失関数 : Loss Function\n",
    "データが与えられただけでは、その予測結果と実際の結果があっているのかどうかが不明。  \n",
    "そこで、損失関数を導入することで、  \n",
    "推論した結果と実際の結果の誤差の大きさを測定することで、予測精度を上げていく。  \n",
    "\n",
    "一般的な損失関数としては、  \n",
    "回帰タスクでは、`nn.MSELoss(Mean Square Error)`  \n",
    "分類タスクでは、`nn.NLLLOSS(Negative Log Likelihood)`が使用される。  \n",
    "`nn.CrossEntropyLoss`は、`nn.LogSoftmax` と`nn.NLLLoss`を結合した損失関数となる。  \n",
    "モデルが出力する`logits`を`nn.CrossEntropyLoss`に与えて正規化し、予測誤差を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の初期化\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化 : Optimizer\n",
    "最適化は各訓練ステップにおいてモデルの誤差を小さくするようにパラメータを調整するステップ  \n",
    "\n",
    "# 最適化アルゴリズム : Optimization algorithms\n",
    "最適化アルゴリズムとは、最適化プロセスの具体的な手続きのこと。  \n",
    "チュートリアルでは、確率的勾配降下法(`SGD : Stochastic Gradient Descent`)を用いる。  \n",
    "最適化のロジックは全て`optimizer`オブジェクト内に含まれている。  \n",
    "最適化の詳細については[最適化の詳細](https://pytorch.org/docs/stable/optim.html)を参照  \n",
    "\n",
    "# optimizerの初期化\n",
    "1. 訓練したいモデルパラメータをoptimizerに登録(渡す)する  \n",
    "2. 同時に、学習率をハイパーパラメータとして渡す。  \n",
    "\n",
    "# optimizerの実装\n",
    "初期化後、最適化(optimizer)は3つのステップから構成される。  \n",
    "1. `optimizer.zero_grad()`を実行し、モデルパラメータの勾配をリセットする。  \n",
    "    > 勾配の計算は蓄積されていくので、epochごとに明示的にリセットする\n",
    "2. 次に、`loss.backward()`を実行し、バックプロパゲーションを実行する。  \n",
    "    > 損失に対する各ぱらめーたの勾配を求める。\n",
    "3. 最後に、`optimizer.step()`を実行し、パラメータを更新する。\n",
    "    > 各パラメータの勾配を用いて、値を更新する。\n",
    "\n",
    "# 実装について\n",
    "最適化を実行するコードをループする関数として、`train`を、  \n",
    "テストデータに対する予測をする関数として、`test`を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # 予測と損失の計算\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss : {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # testデータでは勾配は不要\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行\n",
    "損失関数とoptimizerを初期化し、train,testそれぞれに渡す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 詳細\n",
    "損失関数や最適化などの詳細は、  \n",
    "[Loss Function](https://pytorch.org/docs/stable/nn.html#loss-functions)  \n",
    "[torch.optim](https://pytorch.org/docs/stable/optim.html)  \n",
    "[Warmstart Trainig a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)  \n",
    "を参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f5b1a9f36cac5504d3212872bcc323699452b76dcccf68776354c7e475628a4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
