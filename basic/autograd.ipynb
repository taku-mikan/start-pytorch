{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自動微分\n",
    "Pytorchで、計算履歴を保存し、自動で微分操作を実現するAutoGrad(自動微分)の説明。\n",
    "\n",
    "# Automatic Differentiation with `torch.autograd`\n",
    "ニューラルネットワークを訓練する際に、その学習アルゴリズムとして、  \n",
    "基本的にbackpropagationが使用される。  \n",
    "このとき、モデルの重みやパラメータの損失関数に対する微分値に応じて調整される。  \n",
    "これらの勾配を自動的に計算する仕組みが自動微分であり、`torch.autograd`に組み込まれている。\n",
    "\n",
    "以下、具体例として  \n",
    "入力を`x`、パラメータを`w`, `b`とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5,3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テンソル、関数、計算グラフの関係\n",
    "上記のコードは次の計算グラフ(**computational graph)**を示している。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" width=50% alt=\"計算グラフ\" title=\"計算グラフ\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のネットーワークのうち、`w`, `b`が最適化したいパラメータとなる。  \n",
    "そのために、**これらの変数に対する損失関数の微分**を計算する必要がある。  \n",
    "これらのパラメータでの微分を計算するために、`requires_grad=True`を設定する必要がある。  \n",
    "\n",
    "#### 補足\n",
    "`requires_grad`は、\n",
    "1. テンソルを定義する際、引数に`requires_grad=True`と指定する。  \n",
    "2. 変数定義後、`x.requires_grad_(True)`を実行する。  \n",
    "\n",
    "の二つの方法のどちらかで設定する必要がある。  \n",
    "\n",
    "#### 補足\n",
    "計算グラフを構築する際に、テンソルに適用する関数は、実際には`Function`クラスのオブジェクト。  \n",
    "これらのオブジェクトでは、以下の二つが定義されている。  \n",
    "1. 順伝播時に、入力をどのように処理するのか。  \n",
    "2. 逆伝播時に、勾配をどのように計算するのか。  \n",
    "\n",
    "さらに、勾配は、テンソルの`grad_fn`プロパティに格納されている。  \n",
    "`Function`の詳細は、[Functionの詳細](https://pytorch.org/docs/stable/autograd.html#function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient function for z = \", z.grad_fn)\n",
    "print(\"Gradient function for loss = \", loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配の計算\n",
    "ニューラルネットワークの各パラメータを最適化することを考える。  \n",
    "入力x, 出力yが与えられたもとで、損失関数の各変数の偏微分  \n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w}$\n",
    "$\\frac{\\partial loss}{\\partial b}$  \n",
    "\n",
    "を求める必要がある。  \n",
    "**これらの偏微分を求める**ために、`loss.backward()`を実行し、  \n",
    "各変数の偏微分の値、`w.grad`と`b.grad`を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 補足\n",
    "`grad`が求められるのは以下の二つを満たす時のみ。  \n",
    "1. 計算グラフのleaf nodeであること  \n",
    "2. その変数の`requires_grad=True`であること  \n",
    "\n",
    "全ての変数で勾配が計算可能でないことに注意。  \n",
    "\n",
    "#### 補足\n",
    "勾配の計算は、各計算グラフに対して、**`backward`を実行後、一度のみ**計算できる。  \n",
    "同じ計算グラフに対して、複数回の`backward`を実行したい場合には、  \n",
    "`backward`実行時に、`retain_graph=True`を引数として渡す必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配計算が不必要なとき\n",
    "デフォルトでは、`requires_grad=True`である全てのテンソルは計算履歴が保持され、微分計算が可能。  \n",
    "しかし、訓練済みのモデルでの推論時や順伝播のみを行うときなど、計算が不要。  \n",
    "\n",
    "実装コードで勾配計算を不要にするには以下の二つの方法がある  \n",
    "1. `torch.no_grad()`のブロック内に計算を記述する。  \n",
    "2. テンソルに`detach()`を使用する  \n",
    "\n",
    "実際に、勾配の計算を不要にするケースは以下のようなときがある。  \n",
    "1. ネットワークの一部のパラメータを固定したい\n",
    "> ファインチューニング時によくあるケース  \n",
    "2. 順伝播の計算スピードを高速化したいケース  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 計算グラフの補足\n",
    "複雑な内容だったので、元記事を一旦丸写ししておく..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフについて補足\n",
    "----------------------------\n",
    "\n",
    "概念的には、autogradはテンソルとそれらに対する演算処理を[`Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)を構成単位として、DAG（a directed acyclic graph）の形で保存したグラフです。\n",
    "\n",
    "\n",
    "DAGにおいて、各leafは入力テンソル、そしてrootは出力テンソルです。\n",
    "\n",
    "このグラフをrootから各leafまでchain rule（微分の連鎖律）で追跡することによって各変数に対する偏微分の値を求めることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "順伝搬では autograd は2つの処理を同時に行っています。\n",
    "\n",
    "- 指定された演算を実行し、計算結果のテンソルを求める\n",
    "- DAGの各操作の*gradient function* を更新する\n",
    "\n",
    "<br>\n",
    "\n",
    "逆伝搬では、``.backward()``がDAGのrootのテンソルに対して実行されると、autogradは、\n",
    "\n",
    "- 各変数の ``.grad_fn``を計算する\n",
    "- 各変数の``.grad``属性に微分値を代入する\n",
    "- 微分の連鎖律を使用して、各leafのテンソルの微分値を求める\n",
    "\n",
    "を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【注意】\n",
    "\n",
    "PyTorchではDAGは動的です（Functionで計算処理される際に逐次構築されていきます）。\n",
    "\n",
    "そして、`.backward()`を呼び出すたびに、autogradは再度新しいグラフを作成します。\n",
    "\n",
    "この特性こそが、モデルの順伝搬時に制御フロー文（if文やfor文）を使える理由であり、必要に応じて反復ごとに形や大きさ、操作を変えることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【日本語訳注】\n",
    "\n",
    "上記内容は初心者の方には非常に難しい話です。\n",
    "\n",
    "PyTorchは Define-by-run 形式であり、事前に計算グラフを定義するのではなく、計算を実行する際に、柔軟に計算グラフを作ってくれます。\n",
    "\n",
    "一方で、Define-and-run形式のディープラーニングフレームワーク（例えば、TensorFlow v1など）は、事前に計算グラフを定義する必要があるため、for文やif文といった制御フローの構文を柔軟に使いづらいです。\n",
    "\n",
    "このことを上記内容では説明しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補注：テンソルに対する勾配とヤコビ行列\n",
    "--------------------------------------\n",
    "\n",
    "多くの場合、スカラー値を出力する損失関数に対して、とある変数の勾配を計算します。\n",
    "\n",
    "ですが、関数の出力がスカラー値ではなく、任意のテンソルであるケースもあります。\n",
    "\n",
    "このような場合、PyTorchでは実際の勾配ではなく、いわゆるヤコビ行列（Jacobian\n",
    "matrix）を計算することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ベクトル関数 $\\vec{y}=f(\\vec{x})$,において、\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ 、そして　$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$の場合、\n",
    "\n",
    "その勾配、 $\\vec{y}$ with respect to $\\vec{x}$ はヤコビ行列 で与えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ヤコビ行列そのものを計算する代わりにPyTorchでは、**Jacobian Product**、 $v^T\\cdot J$ を、入力ベクトル$v=(v_1 \\dots v_m)$ に対して計算します。\n",
    "\n",
    "これは、$v$を引数として ``backward``メソッドを呼び出すことで計算されます。\n",
    "\n",
    "なお$v$ のサイズは積を計算したい、元のテンソルの大きさと同じである必要があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記において、同じ変数`inp`に対して、``backward``を2回目実行した際には、勾配が異なる値になった点に注意してください。\n",
    "\n",
    "これはPyTorchでは``backward``を実行すると、勾配を蓄積（accumulate）する仕様だからです。\n",
    "\n",
    "すなわち、計算グラフの全leafの``grad``には、勾配が足し算されます。\n",
    "\n",
    "<br>\n",
    "\n",
    "そのため適切に勾配を計算するには、``grad``を事前に0にリセットする必要があります。\n",
    "\n",
    "なお実際にPyTorchでディープラーニングモデルの訓練を行う際には、**オプティマイザー（optimizer）**が、勾配をリセットする役割を担ってくれます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【注意】\n",
    "\n",
    "本チュートリアルの最初の方で、``backward()``関数を引数のパラメータなしに実行していました。\n",
    "\n",
    "これは実質的には、``backward(torch.tensor(1.0))``を実行しているのと同じとなります。\n",
    "\n",
    "``backward()``はスカラー値の関数（例えば訓練時の損失関数）に対して、各パラメータの勾配を計算する際に便利です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詳細\n",
    "より詳細には、[Aitograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)を参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f5b1a9f36cac5504d3212872bcc323699452b76dcccf68776354c7e475628a4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
